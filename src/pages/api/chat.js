import { GoogleGenerativeAI } from '@google/generative-ai';

const genAI = new GoogleGenerativeAI(import.meta.env.GEMINI_API_KEY);

// üéØ L√çMITES GENEROSOS (gracias a Gemini)
const LIMITS = {
  MAX_MESSAGES_PER_SESSION: 30,    // 30 mensajes por sesi√≥n ‚ú®
  MAX_INPUT_LENGTH: 1000,          // 1000 caracteres por mensaje
  RATE_LIMIT_WINDOW: 60000,        // 1 minuto
  MAX_REQUESTS_PER_MINUTE: 10,     // 10 requests por minuto
};

// Store en memoria para rate limiting
const requestTracker = new Map();

function checkRateLimit(ip) {
  const now = Date.now();
  const userRequests = requestTracker.get(ip) || [];
  
  const recentRequests = userRequests.filter(
    timestamp => now - timestamp < LIMITS.RATE_LIMIT_WINDOW
  );
  
  if (recentRequests.length >= LIMITS.MAX_REQUESTS_PER_MINUTE) {
    return false;
  }
  
  recentRequests.push(now);
  requestTracker.set(ip, recentRequests);
  
  return true;
}

// Limpiar tracker cada 5 minutos
setInterval(() => {
  const now = Date.now();
  for (const [ip, requests] of requestTracker.entries()) {
    const recentRequests = requests.filter(
      timestamp => now - timestamp < LIMITS.RATE_LIMIT_WINDOW
    );
    if (recentRequests.length === 0) {
      requestTracker.delete(ip);
    } else {
      requestTracker.set(ip, recentRequests);
    }
  }
}, 300000);

export async function POST({ request, clientAddress }) {
  try {
    // Rate Limiting
    const ip = clientAddress || 'unknown';
    if (!checkRateLimit(ip)) {
      return new Response(JSON.stringify({
        error: 'L√≠mite de requests alcanzado',
        message: 'Por favor espera un momento antes de continuar. ‚è±Ô∏è'
      }), {
        status: 429,
        headers: { 'Content-Type': 'application/json' }
      });
    }

    const { messages } = await request.json();
    
    // Verificar l√≠mite de mensajes por sesi√≥n
    if (messages.length > LIMITS.MAX_MESSAGES_PER_SESSION) {
      return new Response(JSON.stringify({
        error: 'Sesi√≥n muy larga',
        message: 'Has alcanzado el l√≠mite de 30 mensajes por sesi√≥n. Por favor inicia una nueva conversaci√≥n. üîÑ'
      }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' }
      });
    }

    // Verificar tama√±o del √∫ltimo mensaje
    const lastUserMessage = messages[messages.length - 1];
    if (lastUserMessage.content.length > LIMITS.MAX_INPUT_LENGTH) {
      return new Response(JSON.stringify({
        error: 'Mensaje muy largo',
        message: `Por favor env√≠a mensajes de m√°ximo ${LIMITS.MAX_INPUT_LENGTH} caracteres. üìù`
      }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' }
      });
    }

    // Inicializar el modelo
    const model = genAI.getGenerativeModel({ 
      model: import.meta.env.GEMINI_MODEL || 'gemini-1.5-flash',
      systemInstruction: `Eres "CodePrep AI", un asistente experto en preparaci√≥n para entrevistas t√©cnicas de desarrollo frontend. 

Tu objetivo es ayudar a candidatos a prepararse para entrevistas laborales mediante:

1. Simulaci√≥n de preguntas t√©cnicas y conductuales
2. An√°lisis de respuestas con feedback constructivo y detallado
3. Sugerencias de mejora espec√≠ficas y ejemplos pr√°cticos
4. Estructuraci√≥n de respuestas usando el m√©todo STAR

IMPORTANTE: 
- S√© profesional pero cercano
- Da feedback constructivo, nunca cr√≠tico
- Usa formato claro con emojis: ‚úÖ (bien), ‚ö†Ô∏è (mejorar), üí° (ejemplo)
- Proporciona ejemplos concretos y accionables
- Adapta las preguntas al stack tecnol√≥gico mencionado
- Si es el primer mensaje, pregunta para qu√© empresa/posici√≥n se prepara

METODOLOG√çA para feedback:
‚úÖ QU√â ESTUVO BIEN: [aspectos positivos espec√≠ficos]
‚ö†Ô∏è PARA MEJORAR: [√°reas de oportunidad concretas]
üí° RESPUESTA MEJORADA: [ejemplo estructurado y completo]
üìö RECURSOS: [si aplica, sugerir qu√© estudiar]`
    });

    // Convertir mensajes al formato de Gemini
    const chatHistory = messages.slice(0, -1).map(msg => ({
      role: msg.role === 'user' ? 'user' : 'model',
      parts: [{ text: msg.content }]
    }));

    const chat = model.startChat({
      history: chatHistory,
      generationConfig: {
        maxOutputTokens: 1024,
        temperature: 0.7,
      },
    });

    // Enviar el √∫ltimo mensaje
    const result = await chat.sendMessage(lastUserMessage.content);
    const response = await result.response;
    const text = response.text();

    // Log para monitoreo
    console.log(`[Gemini API] IP: ${ip}, Messages: ${messages.length}`);

    return new Response(JSON.stringify({
      content: text
    }), {
      status: 200,
      headers: { 'Content-Type': 'application/json' }
    });

  } catch (error) {
    console.error('Error:', error);
    
    // Manejar errores espec√≠ficos
    if (error.message?.includes('quota') || error.message?.includes('limit')) {
      return new Response(JSON.stringify({
        error: 'L√≠mite de API alcanzado',
        message: 'El servicio ha alcanzado su l√≠mite temporalmente. Por favor intenta en unos minutos. üïê'
      }), {
        status: 429,
        headers: { 'Content-Type': 'application/json' }
      });
    }

    return new Response(JSON.stringify({
      error: 'Error al procesar la solicitud',
      message: 'Ocurri√≥ un error. Por favor intenta nuevamente.',
      details: import.meta.env.DEV ? error.message : undefined
    }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' }
    });
  }
}